# General RAG system configuration
general:
  debug: false
  log_dir: "logs"
  sessions_dir: "sessions"
  log_level: "INFO"
  stream_response: true

# Session management
sessions:
  max_sessions: 50
  timeout: 604800  # 7 days
  cleanup_interval: 300  # 5 minutes
  max_contexts: 50

# Document chunking
chunks:
  # Method: character, token, context, page
  method: "token" 
  
  # Header format: standard (Markdown) or simple
  header_format: "simple"
  
  memory_optimization:
    enabled: true
    batch_size: 50
    min_batch_size: 5
    max_batch_size: 200
    memory_check_interval: 15
    gc_min_interval: 60
    force_gc: true
    memory_monitor: true
    advanced:
      small_doc_batch_factor: 1.5
      large_doc_batch_factor: 0.7
      small_doc_threshold_kb: 50
      large_doc_threshold_kb: 1000
  
  character:
    chunk_size: 1000
    chunk_overlap: 200
    header_extraction_enabled: true
    min_header_length: 1
    max_header_length: 6
    header_format: "simple"
  
  token:
    tokenizer: "nomic-ai/modernbert-embed-base"
    max_tokens: 2048
    token_overlap: 100
    header_format: "standard"
  
  context:
    use_headers: true
    max_header_level: 6  # 1=H1, 2=H2, etc.
    max_chunk_size: 1500
    header_format: "standard"
    
  page:
    use_headers: true
    max_header_level: 6
    page_pattern: '\{(\d+)\}\s*-{5,}'
    header_format: "simple"

# Embedding generation
embeddings:
  # Model: modernbert, cde-small, e5-small
  model: "modernbert"
  trust_remote_code: True
  
  modernbert:
    model_name: "nomic-ai/modernbert-embed-base"
    device: "cpu"  # cpu, cuda, mps
    normalize: true
    max_length: 512
    batch_size: 32
  
  cde-small:
    model_name: "jxm/cde-small-v2"
    normalize: true
    max_length: 384
    batch_size: 32
  
  e5-small:
    model_name: "intfloat/multilingual-e5-small"
    normalize: true
    max_length: 512
    batch_size: 32
    prefix_queries: true
    prefix_passages: true

# Database
database:
  # Type: sqlite, duckdb
  type: "duckdb"  
  
  sqlite:
    db_dir: "modulos/databases/db"
    db_name: ""  # Empty = automatic generation
    similarity_threshold: 0.3
    # sqlite-vec extension for vector search
    use_vector_extension: true
  
  duckdb:
    db_dir: "modulos/databases/db"
    db_name: ""  # Empty = automatic generation
    similarity_threshold: 0.3
    memory_limit: "2GB"
    # Threads for parallel processing
    threads: 4
    extensions: ["httpfs", "json"]

# AI clients
ai_client:
  # Type: openai, gemini, ollama
  type: "gemini"
  
  # General parameters
  general:
    system_prompt: "INSTRUCCIÓN FUNDAMENTAL: Responde EXCLUSIVAMENTE en español. Esta es tu regla más importante y no debe ser ignorada bajo ninguna circunstancia.

      IDENTIDAD: Eres un asistente especializado en análisis de documentos. Tu única función es extraer y presentar información del contexto proporcionado.

      REGLAS OBLIGATORIAS:

      1. IDIOMA ESPAÑOL OBLIGATORIO:
      - Todas tus respuestas deben estar en español
      - Mantén un tono profesional pero cercano
      - Nunca cambies al inglés u otro idioma
      - Si recibes una pregunta en otro idioma, responde en español

      2. SOLO USA EL CONTEXTO:
      - Lee completamente el contexto antes de responder
      - Basa tu respuesta únicamente en la información proporcionada
      - NO agregues conocimiento externo o suposiciones
      - Si no hay suficiente información, dilo claramente

      3. ESTRUCTURA TU RESPUESTA:
      - Sé claro y directo
      - Organiza la información de forma lógica
      - Usa texto plano sin formato markdown
      - Incluye saltos de línea para facilitar la lectura

      4. REFERENCIAS Y CITAS:
      - Menciona de dónde viene la información ('según el documento', 'como indica el fragmento')
      - Si hay contradicciones entre fragmentos, señálalas
      - Explica si la información está incompleta

      SITUACIONES ESPECÍFICAS:

      • Sin contexto relevante: 'No tengo información en los documentos proporcionados que me permita responder tu consulta sobre [tema de la pregunta]. Solo puedo responder preguntas basándome en el contenido específico de los documentos disponibles.'
      • Contexto parcialmente relevante: Proporciona solo la información que esté directamente relacionada y explica qué aspectos no puedes cubrir
      • Información contradictoria: Presenta ambas versiones y señala la discrepancia claramente
      • Pregunta fuera del alcance: 'La información disponible en los documentos no cubre el tema de tu consulta. Solo puedo ayudarte con preguntas relacionadas con el contenido específico de estos documentos.'
      • Conversación interactiva: Mantente siempre enfocado en el contexto documental, nunca uses conocimiento general

      EVALUACIÓN DE RELEVANCIA:
      Antes de responder, evalúa si el contexto proporcionado tiene relación directa con la pregunta:
      - Si NO hay relación: Usa las frases de 'sin contexto relevante' o 'fuera del alcance'
      - Si hay relación parcial: Responde solo lo que puedas con el contexto disponible
      - Si hay relación completa: Responde basándote completamente en el contexto

      RECORDATORIO FINAL: Tu valor está en ser preciso, confiable y estar basado completamente en el material proporcionado. Nunca inventes información. Siempre en español."
    
    # RAG-optimized parameters
    # Response creativity
    temperature: 0.7
    # Maximum tokens
    max_tokens: 2048
    # Top-p (nucleus sampling)
    top_p: 0.85
    # Top-k (controlled diversity)
    top_k: 50
    # Real-time streaming
    stream: false
    # Response format
    response_mime_type: "text/plain"
    context_format: "fragments"  # fragments, simple, numbered
    instruction_style: "detailed"  # minimal, detailed, standard
  
  openai:
    model: "gpt-3.5-turbo"
    api_key_env: "OPENAI_API_KEY"
    api_base_env: "OPENAI_API_BASE"
    embedding_model: "text-embedding-ada-002"
    # Specific parameters
    frequency_penalty: 0.0
    presence_penalty: 0.0
    timeout: 30
  
  gemini:
    # Model: gemini-2.0-flash, gemma-3-1b-it, gemma-3-4b-it, gemma-3-12b-it, gemma-3-27b-it
    model: "gemma-3-1b-it"
    api_key_env: "GEMINI_API_KEY"
    embedding_model: "embedding-001"
  
  ollama:
    model: "gemma3:1b"
    api_url: "http://localhost:11434"
    # Environment variable for URL (optional)
    api_url_env: "OLLAMA_API_URL"
    embedding_model: "all-minilm"
    timeout: 120

# Document processing
processing:
  max_chunks_to_retrieve: 5

# Resource management
resource_management:
  log_verbosity: "normal"  # minimal, normal, detailed
  
  monitoring:
    interval_sec: 120  # 0 = disabled
    suspended_interval_sec: 300
    aggressive_threshold_mem_pct: 85
    warning_threshold_mem_pct: 75
    warning_threshold_cpu_pct: 90
    min_cleanup_interval_sec: 60
    min_aggressive_cleanup_interval_sec: 300
    min_memory_change_pct: 3
    extended_cooling_period_sec: 600
  
  memory:
    auto_suspend_memory_mb: 1000
    max_suspend_duration_sec: 1800
    document_size_threshold_kb: 5000
    min_chunks_for_suspend: 200
    model_release:
      inactive_timeout_sec: 300
      min_activity_for_force_release_sec: 60
      max_failed_cleanups_before_extreme: 3
      force_release_memory_threshold_pct: 90
  
  concurrency:
    # CPU Workers ("auto" = automatic)
    cpu_workers: "auto"
    # I/O Workers ("auto" = automatic)
    io_workers: "auto"
    # Total workers limit (null = no limit)
    max_total_workers: null
    disable_process_pool: false
    default_timeout_sec: 120
  
  worker_management:
    worker_recalculation_frequency: 300.0
    dynamic_recalc_interval: true
    max_recalc_interval: 1800.0
    worker_cooldown_after_change: 120.0
    worker_change_threshold_pct: 15
  
  environment:
    environment_type: ""  # development, production, server, cloud, container (empty = auto)
    override_environment_adjustments: false

# Garbage collection
gc:
  adaptive_gc_enabled: true
  gc_objects_threshold: 1000
  adjust_thresholds: true
  auto_restore_thresholds: true
  threshold_restore_delay_sec: 60
  temporary_thresholds: [700, 10, 10]  # gen0, gen1, gen2
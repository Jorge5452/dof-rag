# Configuración general del sistema RAG
general:
  # Modo de depuración (activa logs más detallados)
  debug: false
  # Directorio para almacenar logs
  log_dir: "logs"
  # Directorio para almacenar sesiones
  sessions_dir: "sessions"
  # Nivel de logging (DEBUG, INFO, WARNING, ERROR)
  log_level: "INFO"
  stream_response: true

# Configuración para gestión de sesiones
sessions:
  max_sessions: 50
  timeout: 604800  # 7 días en segundos
  cleanup_interval: 300  # 5 minutos
  max_contexts: 50

# Configuración para chunking de documentos
chunks:
  # Método de chunking a utilizar (character, token, context, page)
  method: "token" 
  
  # Formato de encabezado por defecto (standard, simple)
  # standard: Formato detallado con encabezados Markdown
  # simple: Formato simple tipo "Documento - Página X - Encabezado"
  header_format: "simple"
  
  # Configuración para memoria y procesamiento
  memory_optimization:
    # Habilitar optimización de memoria (procesamiento de chunks uno por uno)
    enabled: true
    # Tamaño de lote base para proceso de chunks (será optimizado dinámicamente)
    batch_size: 50
    # Tamaño mínimo de lote permitido (para prevenir rendimiento pobre)
    min_batch_size: 5
    # Tamaño máximo de lote permitido (para prevenir problemas de memoria)
    max_batch_size: 200
    # Intervalos para verificación de memoria durante procesamiento (segundos)
    memory_check_interval: 15
    # Intervalo mínimo entre operaciones GC (segundos)
    gc_min_interval: 60
    # Forzar garbage collection periódicamente
    force_gc: true
    # Monitorear uso de memoria
    memory_monitor: true
    # Configuraciones avanzadas
    advanced:
      # Aumentar el batch size para documentos pequeños (factor)
      small_doc_batch_factor: 1.5
      # Reducir el batch size para documentos grandes (factor)
      large_doc_batch_factor: 0.7
      # Umbral para considerar documento pequeño (KB)
      small_doc_threshold_kb: 50
      # Umbral para considerar documento grande (KB)
      large_doc_threshold_kb: 1000
  
  # Configuración para chunker por caracteres
  character:
    # Tamaño máximo del chunk en caracteres
    chunk_size: 1000
    # Solapamiento entre chunks en caracteres
    chunk_overlap: 200
    # Habilitar o deshabilitar extracción de encabezados (puede mejorar rendimiento)
    header_extraction_enabled: true
    # Longitud mínima para considerar un texto como encabezado
    min_header_length: 1
    # Longitud máxima para un encabezado (textos más largos se saltan para mejorar rendimiento)
    max_header_length: 6
    # Formato de encabezado específico para este método (opcional)
    header_format: "simple"
  
  # Configuración para chunker por tokens
  token:
    # Nombre del modelo de tokenizador (debe ser compatible con HF)
    # tokenizer: "intfloat/multilingual-e5-small"
    tokenizer: "nomic-ai/modernbert-embed-base"
    # Cantidad máxima de tokens por chunk
    max_tokens: 2048
    # Solapamiento entre chunks en tokens
    token_overlap: 100
    # Formato de encabezado específico para este método (opcional)
    header_format: "standard"
  
  # Configuración para chunker por contexto
  context:
    # Utilizar encabezados para dividir chunks
    use_headers: true
    # Nivel máximo de encabezado para separar (1=H1, 2=H2, etc.)
    max_header_level: 6
    # Tamaño máximo de texto por chunk
    max_chunk_size: 1500
    # Formato de encabezado específico para este método (opcional)
    header_format: "standard"
    
  # Configuración para chunker por páginas
  page:
    # Utilizar encabezados para mejorar el contexto
    use_headers: true
    # Nivel máximo de encabezado a considerar
    max_header_level: 6
    # Patrón de expresión regular para identificar marcadores de página
    page_pattern: '\{(\d+)\}\s*-{5,}'
    # Formato de encabezado específico para este método (opcional)
    header_format: "simple"

# Configuración para generación de embeddings
embeddings:
  # Modelo de embedding a utilizar (modernbert, cde-small, e5-small)
  model: "modernbert"
  # Incluir parámetro para trust_remote_code
  trust_remote_code: True
  
  # Configuración para ModernBERT
  modernbert:
    # Nombre completo del modelo
    model_name: "nomic-ai/modernbert-embed-base"
    # Dispositivo para inferencia (cpu, cuda, mps)
    device: "cpu"
    # Normalizar embeddings para similitud por coseno
    normalize: true
    # Longitud máxima de tokens para el modelo
    max_length: 512
    # Nota: El procesamiento por lotes está deshabilitado para evitar problemas de memoria
    # Este parámetro se mantiene por retrocompatibilidad pero no se usa
    batch_size: 32
  
  # Configuración para CDE-small
  cde-small:
    model_name: "jxm/cde-small-v2"
    normalize: true
    max_length: 384
    # Nota: El procesamiento por lotes está deshabilitado para evitar problemas de memoria
    batch_size: 32
  
  # Configuración para E5-small
  e5-small:
    model_name: "intfloat/multilingual-e5-small"
    normalize: true
    max_length: 512
    # Nota: El procesamiento por lotes está deshabilitado para evitar problemas de memoria
    batch_size: 32
    prefix_queries: true
    prefix_passages: true

# Configuración de la base de datos
database:
  # Tipo de base de datos (sqlite, duckdb, ...)
  type: "duckdb"
  
  # Configuración para SQLite
  sqlite:
    # Ruta al archivo de base de datos
    db_dir: "modulos/databases/db"
    db_name: ""  # Nombre vacío para que siempre se genere automáticamente
    similarity_threshold: 0.3
    # Habilitar extensión sqlite-vec para búsqueda vectorial optimizada
    use_vector_extension: true
  
  # Configuración para DuckDB
  duckdb:
    # Ruta al archivo de base de datos
    db_dir: "modulos/databases/db"
    db_name: ""  # Nombre vacío para que siempre se genere automáticamente
    similarity_threshold: 0.3
    # Límite de memoria para operaciones de DuckDB (2GB por defecto)
    memory_limit: "2GB"
    # Número de hilos para procesamiento paralelo (4 es un valor seguro para la mayoría de sistemas)
    threads: 4
    # Extensiones a instalar (httpfs, json)
    extensions: ["httpfs", "json"]

# Configuración para clientes de IA
ai_client:
  # Tipo de cliente por defecto (openai, gemini, ollama)
  type: "gemini"
  
  # Parámetros generales para modelos de IA (aplicables a todos los modelos)
  general:
    # Prompt inicial de sistema mejorado y detallado
    system_prompt: "Eres un asistente de IA especializado en análisis de documentos y respuestas basadas en contexto. Tu función principal es analizar cuidadosamente la información proporcionada y extraer respuestas precisas y relevantes.

      INSTRUCCIONES PRINCIPALES:
      1. ANÁLISIS DEL CONTEXTO: Examina meticulosamente toda la información del contexto proporcionado antes de formular tu respuesta.

      2. RESPUESTAS BASADAS EN EVIDENCIA: Basa tus respuestas ÚNICAMENTE en la información presente en el contexto. No añadas información externa, suposiciones o conocimiento general que no esté respaldado por el material proporcionado.

      3. PRECISIÓN Y CLARIDAD: Responde de manera clara, concisa y profesional. Estructura tu respuesta de forma lógica y fácil de entender.

      4. MANEJO DE INFORMACIÓN INSUFICIENTE: Si el contexto no contiene información suficiente para responder completamente a la pregunta, indícalo claramente. Es preferible una respuesta parcial honesta que una respuesta completa pero especulativa.

      5. IDIOMA: Responde SIEMPRE en español, manteniendo un tono profesional pero accesible.

      6. FORMATO DE RESPUESTA: Presenta la información de manera clara y bien estructurada. Evita usar formato markdown en tu respuesta; utiliza texto plano con saltos de línea y estructura natural para facilitar la lectura.

      7. CITAS Y REFERENCIAS: Cuando sea posible, menciona de qué parte del contexto proviene la información (por ejemplo, 'según el documento', 'como se indica en el fragmento', etc.).

      8. COHERENCIA: Asegúrate de que tu respuesta sea coherente y no contenga contradicciones internas.

      FORMATO DE CONTEXTO:
      Recibirás el contexto organizado en fragmentos numerados. Cada fragmento puede incluir encabezados y contenido específico del documento.

      CASOS ESPECIALES:
      - Sin contexto: Si no recibes contexto o está vacío, responde de manera amable indicando que no tienes información específica disponible para responder a la consulta.
      - Contexto parcial: Si el contexto es limitado, proporciona la información disponible y especifica qué aspectos de la pregunta no pueden ser respondidos con la información actual.
      - Información contradictoria: Si encuentras información contradictoria en diferentes fragmentos, señala estas discrepancias y presenta ambas perspectivas.

      Recuerda: Tu valor está en proporcionar respuestas precisas, confiables y bien fundamentadas en el material proporcionado."
    
    # Parámetros optimizados para RAG (aplicables a todos los modelos)
    # Temperatura más maleable para respuestas más naturales
    temperature: 0.7
    # Tokens máximos para la respuesta
    max_tokens: 2048
    # Top-p para muestreo (nucleus sampling) más diverso
    top_p: 0.85
    # Top-k para diversidad controlada más amplia
    top_k: 50
    # Utilizar streaming (tiempo-real) para respuestas
    stream: false
    # Formato de respuesta
    response_mime_type: "text/plain"
    # Instrucciones para el formateo de contexto
    context_format: "fragments"  # options: fragments, simple, numbered
    # Estilo de formateo de instrucciones
    instruction_style: "detailed"  # options: minimal, detailed, standard
  
  # Configuración específica para OpenAI (solo lo específico de la plataforma)
  openai:
    # Modelo a utilizar
    model: "gpt-3.5-turbo"
    # Nombre de la variable de entorno para la API key
    api_key_env: "OPENAI_API_KEY"
    # Nombre de la variable de entorno para la URL base de la API (opcional)
    api_base_env: "OPENAI_API_BASE"
    # Modelo para embeddings
    embedding_model: "text-embedding-ada-002"
    # Parámetros únicos para OpenAI (que no estén en general)
    frequency_penalty: 0.0
    presence_penalty: 0.0
    timeout: 30
  
  # Configuración específica para Gemini
  gemini:
    # Modelo a utilizar
    model: "gemini-2.0-flash"
    # Nombre de la variable de entorno para la API key
    api_key_env: "GEMINI_API_KEY"
    # Modelo para embeddings
    embedding_model: "embedding-001"
  
  # Configuración específica para Ollama
  ollama:
    # Modelo a utilizar
    model: "gemma3:1b"
    # URL de la API
    api_url: "http://localhost:11434"
    # Nombre de la variable de entorno para la URL de la API (opcional)
    api_url_env: "OLLAMA_API_URL"
    # Modelo para embeddings
    embedding_model: "all-minilm"
    # Timeout específico para Ollama
    timeout: 60

# Parámetros para procesamiento de documentos
processing:
  # Número máximo de chunks a recuperar para cada consulta
  max_chunks_to_retrieve: 5

# Configuración para el Gestor de Recursos Centralizado
resource_management:
  # Nivel de verbosidad en los logs (minimal, normal, detailed)
  log_verbosity: "normal"
  
  # Configuración de monitoreo de recursos
  monitoring:
    # Intervalo en segundos para monitoreo de recursos (0 para deshabilitar)
    interval_sec: 120
    # Intervalo alternativo cuando las verificaciones están suspendidas
    suspended_interval_sec: 300
    # Umbral de memoria (%) para limpieza agresiva
    aggressive_threshold_mem_pct: 85
    # Umbral de memoria (%) para advertencia y limpieza estándar
    warning_threshold_mem_pct: 75
    # Umbral de CPU (%) para advertencias
    warning_threshold_cpu_pct: 90
    # Intervalos mínimos entre limpiezas
    min_cleanup_interval_sec: 60
    # Intervalo mínimo entre limpiezas agresivas
    min_aggressive_cleanup_interval_sec: 300
    # Porcentaje mínimo de cambio en memoria para considerar una limpieza efectiva
    min_memory_change_pct: 3
    # Período de enfriamiento extendido después de una limpieza ineficaz (segundos)
    extended_cooling_period_sec: 600
  
  # Configuración de memoria
  memory:
    # Memoria disponible (MB) por debajo de la cual se suspenden verificaciones automáticamente
    auto_suspend_memory_mb: 1000
    # Duración máxima de suspensión en segundos
    max_suspend_duration_sec: 1800
    # Tamaño de documento (KB) por encima del cual considerar suspensión
    document_size_threshold_kb: 5000
    # Número mínimo de chunks para considerar suspensión
    min_chunks_for_suspend: 200
    # NUEVO: Parámetros mejorados para liberación de modelos
    model_release:
      # Tiempo inactivo (segundos) antes de considerar un modelo para liberación
      inactive_timeout_sec: 300
      # Tiempo mínimo (segundos) para poder liberar modelos activos en situación crítica
      min_activity_for_force_release_sec: 60
      # Número máximo de limpiezas fallidas antes de usar medidas extremas
      max_failed_cleanups_before_extreme: 3
      # Forzar liberar modelos activos cuando la memoria supera este porcentaje
      force_release_memory_threshold_pct: 90
  
  # Configuración de concurrencia
  concurrency:
    # Número de workers para tareas intensivas en CPU ("auto" para determinar automáticamente)
    cpu_workers: "auto"
    # Número de workers para tareas intensivas en I/O ("auto" para determinar automáticamente)
    io_workers: "auto"
    # Límite máximo total de workers (null para no limitar)
    max_total_workers: null
    # Deshabilitar el pool de procesos y usar solo hilos
    disable_process_pool: false
    # Timeout predeterminado para tareas en segundos
    default_timeout_sec: 120
  
  # Configuración de gestión de workers
  worker_management:
    # Frecuencia de recálculo de workers en segundos
    worker_recalculation_frequency: 300.0
    # Ajustar dinámicamente el intervalo de recálculo basado en carga
    dynamic_recalc_interval: true
    # Intervalo máximo de recálculo en segundos
    max_recalc_interval: 1800.0
    # Período de enfriamiento después de cambiar workers (segundos)
    worker_cooldown_after_change: 120.0
    # Umbral de cambio de recursos (%) para recalcular workers
    worker_change_threshold_pct: 15
  
  # Configuración de entorno
  environment:
    # Tipo de entorno explícito (development, production, server, cloud, container)
    # Dejar vacío para auto-detectar
    environment_type: ""
    # Si es true, no se aplicarán ajustes automáticos basados en entorno
    override_environment_adjustments: false

# Configuración para garbage collection optimizado
gc:
  # Activar sistema de GC adaptativo
  adaptive_gc_enabled: true
  # Umbral de objetos para GC agresivo
  gc_objects_threshold: 1000
  # Ajustar temporalmente umbrales GC para mejorar recolección
  adjust_thresholds: true
  # Programar restauración de umbrales tras limpieza agresiva
  auto_restore_thresholds: true
  # Restaurar después de segundos
  threshold_restore_delay_sec: 60
  # Umbrales temporales para GC (generación 0, generación 1, generación 2)
  temporary_thresholds: [700, 10, 10]
import os
import logging
import struct
import json
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import uuid

try:
    import duckdb
except ImportError:
    duckdb = None

from modulos.databases.VectorialDatabase import VectorialDatabase

logger = logging.getLogger(__name__)

class DuckDBVectorialDatabase(VectorialDatabase):
    """
    Implementaci√≥n de VectorialDatabase usando DuckDB.
    
    La dimensi√≥n del embedding se fija en la inicializaci√≥n y se utiliza para todas las operaciones
    vectoriales posteriores, garantizando consistencia y mejor rendimiento.
    """
    
    def __init__(self, embedding_dim: int):
        """
        Inicializa la base de datos DuckDB con una dimensi√≥n de embedding espec√≠fica.
        
        Args:
            embedding_dim: Dimensi√≥n fija de los embeddings que se utilizar√°n.
                           Este valor debe venir del modelo de embeddings y es obligatorio.
        """
        super().__init__()  # Inicializar la superclase para heredar atributos como _logger
        
        if duckdb is None:
            raise ImportError("DuckDB no est√° instalado. Inst√°lalo con 'pip install duckdb'.")
        
        if embedding_dim is None or embedding_dim <= 0:
            raise ValueError("La dimensi√≥n del embedding debe ser un n√∫mero positivo")
            
        self._conn = None
        self._ext_loaded = False
        self._schema_created = False
        self._similarity_threshold = 0.3
        self._db_path = None  # A√±adido para guardar la ruta de la base de datos
        self._in_transaction = False  # Flag para rastrear si hay una transacci√≥n activa
        
        # Fijar la dimensi√≥n del embedding
        self._embedding_dim = embedding_dim
        logger.info(f"Dimensi√≥n de embedding fijada en: {self._embedding_dim}")
    
    def connect(self, db_path: str) -> bool:
        """
        Conecta a la base de datos DuckDB.
        
        Args:
            db_path: Ruta al archivo de base de datos
            
        Returns:
            True si la conexi√≥n fue exitosa, False en caso contrario
        """
        try:
            # Guardar la ruta de la base de datos para uso posterior
            self._db_path = db_path
            
            # Leer configuraci√≥n espec√≠fica para DuckDB
            from config import config
            db_config = config.get_database_config()
            duckdb_config = db_config.get('duckdb', {})
            
            # Obtener par√°metros de configuraci√≥n
            memory_limit = duckdb_config.get('memory_limit', '2GB')
            threads_config = duckdb_config.get('threads', 4)  # Default a 4 threads
            self._similarity_threshold = duckdb_config.get('similarity_threshold', 0.3)
            
            # Procesar el valor de threads: DuckDB requiere al menos 1 thread
            import multiprocessing
            if threads_config == 'auto' or threads_config == 0:
                # Usar el n√∫mero de n√∫cleos disponibles, al menos 1
                threads = max(1, multiprocessing.cpu_count())
            else:
                # Intentar convertir a entero, asegurando que sea al menos 1
                try:
                    threads = max(1, int(threads_config))
                except (ValueError, TypeError):
                    # Si no es posible convertir, usar 1 como valor seguro por defecto
                    logger.warning(f"Valor inv√°lido para threads: '{threads_config}'. Usando 1 como valor por defecto.")
                    threads = 1
            
            # Validaci√≥n adicional para asegurar que sea un n√∫mero v√°lido
            if not isinstance(threads, int) or threads < 1:
                threads = 1
                logger.warning(f"Valor corregido de threads a {threads} para cumplir con los requisitos de DuckDB")
            
            # Configuraci√≥n para conexi√≥n DuckDB
            connection_config = {
                'memory_limit': memory_limit,
                'threads': threads  # Debe ser un n√∫mero entero, no string
            }
            
            logger.debug(f"Configuraci√≥n para conexi√≥n DuckDB: {connection_config}")
            
            if db_path == ':memory:':
                self._conn = duckdb.connect(database=':memory:', config=connection_config)
            else:
                # Asegurarse de que el directorio existe
                db_dir = os.path.dirname(os.path.abspath(db_path))
                if not os.path.exists(db_dir):
                    try:
                        os.makedirs(db_dir, exist_ok=True)
                        logger.info(f"Directorio creado para la base de datos: {db_dir}")
                    except Exception as e:
                        logger.error(f"Error al crear directorio para la base de datos: {e}")
                        return False
                
                # Configuraciones mejoradas para DuckDB
                self._conn = duckdb.connect(database=db_path, config=connection_config)
            
            # Verificar que la conexi√≥n se haya establecido correctamente
            if self._conn is None:
                logger.error("La conexi√≥n a DuckDB se cre√≥ como None")
                return False
                
            # Ejecutar una consulta sencilla para verificar que la conexi√≥n funciona
            try:
                self._conn.execute("SELECT 1")
                self._conn.fetchone()
            except Exception as e:
                logger.error(f"La conexi√≥n a DuckDB se cre√≥ pero no es funcional: {e}")
                return False
            
            # Inicializar el cursor para compatibilidad con la clase abstracta
            self._cursor = self._conn
            
            # Cargar extensiones necesarias inmediatamente despu√©s de conectar
            # Pero no fallar si no se pueden cargar
            self.load_extensions()
            
            logger.info(f"Conexi√≥n exitosa a DuckDB: {db_path} (memoria: {memory_limit}, hilos: {threads})")
            
            # Crear el esquema tras conectar exitosamente
            if not self.create_schema():
                logger.warning("No se pudo crear el esquema completo en DuckDB, pero se continuar√° el proceso")
            
            return True
        except Exception as e:
            logger.error(f"Error al conectar a DuckDB: {e}")
            self._conn = None
            self._cursor = None
            return False
    
    def close_connection(self) -> bool:
        """
        Cierra la conexi√≥n a la base de datos.
        
        Returns:
            True si se cerr√≥ correctamente, False en caso contrario
        """
        try:
            if self._conn:
                self._conn.close()
                self._conn = None
                logger.info("Conexi√≥n a DuckDB cerrada correctamente")
            return True
        except Exception as e:
            logger.error(f"Error al cerrar conexi√≥n DuckDB: {e}")
            return False
    
    def create_schema(self) -> bool:
        """
        Crea el esquema de la base de datos si no existe.
        
        Returns:
            True si se cre√≥ correctamente, False en caso contrario
        """
        if not self._conn:
            logger.error("No hay conexi√≥n a la base de datos")
            return False
        
        try:
            # Verificar si las tablas ya existen para evitar errores
            try:
                self._conn.execute("CREATE SCHEMA IF NOT EXISTS main")
            except Exception as e:
                logger.warning(f"No se pudo crear el esquema main (puede ser ignorado): {e}")
            
            # Verificar si ya existen las tablas principales
            try:
                self._conn.execute("SELECT 1 FROM documents LIMIT 1")
                self._conn.execute("SELECT 1 FROM chunks LIMIT 1")
                logger.info("Las tablas ya existen en la base de datos, omitiendo creaci√≥n de esquema")
                self._schema_created = True
                return True
            except Exception:
                # Si hay un error al consultar las tablas, es porque no existen y debemos crearlas
                pass
            
            # Crear secuencia para IDs de documentos
            try:
                self._conn.execute("""
                    CREATE SEQUENCE IF NOT EXISTS doc_id_seq;
                """)
            except Exception as e:
                logger.warning(f"Error al crear secuencia doc_id_seq: {e}")
                # Continuar a pesar del error
            
            # Crear tabla de documentos con ID autogenerado usando la secuencia
            # Usando CAST para asegurar que el tipo sea INTEGER
            self._conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    id INTEGER PRIMARY KEY DEFAULT CAST(nextval('doc_id_seq') AS INTEGER),
                    title TEXT,
                    url TEXT,
                    file_path TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Crear secuencia para IDs de chunks
            try:
                self._conn.execute("""
                    CREATE SEQUENCE IF NOT EXISTS chunk_id_seq;
                """)
            except Exception as e:
                logger.warning(f"Error al crear secuencia chunk_id_seq: {e}")
                # Continuar a pesar del error
            
            # Crear tabla de chunks con ID autogenerado usando la secuencia
            # Usar FLOAT[] para embeddings en lugar de BLOB para compatibilidad con funciones vectoriales
            self._conn.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY DEFAULT CAST(nextval('chunk_id_seq') AS INTEGER),
                    document_id INTEGER,
                    text TEXT NOT NULL,
                    header TEXT,
                    page TEXT,
                    embedding FLOAT[],
                    embedding_dim INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (document_id) REFERENCES documents(id)
                )
            """)
            
            # Crear tabla de metadatos si no existe
            self._conn.execute("""
                CREATE TABLE IF NOT EXISTS db_metadata (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """)
            
            # Crear √≠ndice para b√∫squedas r√°pidas por document_id
            self._conn.execute("CREATE INDEX IF NOT EXISTS idx_chunks_document_id ON chunks(document_id)")
            
            # Guardar metadato sobre la dimensi√≥n de embeddings
            try:
                self.store_metadata("embedding_dim", str(self._embedding_dim))
            except Exception as e:
                logger.warning(f"No se pudo guardar metadata de embedding_dim: {e}")
            
            self._schema_created = True
            logger.info("Esquema de DuckDB creado correctamente")
            return True
        except Exception as e:
            logger.error(f"Error al crear esquema en DuckDB: {e}")
            return False
    
    def load_extensions(self) -> bool:
        """
        Carga extensiones para b√∫squeda vectorial si est√°n disponibles.
        
        Returns:
            True si las extensiones se cargaron correctamente, False en caso contrario.
        """
        # Evitar cargar extensiones si ya est√°n cargadas
        if self._ext_loaded:
            logger.debug("Extensiones DuckDB ya est√°n cargadas")
            return True
            
        # Evitar intentar cargar extensiones si no hay conexi√≥n
        if not self._conn:
            logger.error("No hay conexi√≥n a la base de datos para cargar extensiones")
            return False
            
        try:
            # Leer las extensiones configuradas
            from config import config
            db_config = config.get_database_config()
            duckdb_config = db_config.get('duckdb', {})
            extensions = duckdb_config.get('extensions', [])  # Lista vac√≠a por defecto
            
            # Filtrar expl√≠citamente las extensiones no deseadas
            excluded_extensions = ['httpfs', 'json']
            extensions = [ext for ext in extensions if ext not in excluded_extensions]
            
            logger.info(f"Extensiones a cargar: {extensions}")
            
            # Instalar y cargar las extensiones configuradas
            for extension in extensions:
                try:
                    # Comprobar si la extensi√≥n ya est√° cargada
                    try:
                        self._conn.execute(f"SELECT 1 FROM pragma_installed_extensions() WHERE extension_name = '{extension}'")
                        already_installed = len(self._conn.fetchall()) > 0
                    except:
                        already_installed = False
                    
                    if not already_installed:
                        logger.debug(f"Instalando extensi√≥n DuckDB '{extension}'...")
                        self._conn.execute(f"INSTALL {extension};")
                    
                    logger.debug(f"Cargando extensi√≥n DuckDB '{extension}'...")
                    self._conn.execute(f"LOAD {extension};")
                    logger.info(f"Extensi√≥n DuckDB '{extension}' cargada correctamente")
                except Exception as e:
                    logger.debug(f"No se pudo cargar extensi√≥n '{extension}' (no es cr√≠tico): {e}")
            
            self._ext_loaded = True
            logger.info("Proceso de carga de extensiones DuckDB completado")
            return True
        except Exception as e:
            logger.warning(f"No se pudieron cargar todas las extensiones DuckDB: {e}")
            # No fallar completamente si las extensiones no se cargan
            self._ext_loaded = True
            return True  # Devolver True para permitir que el proceso contin√∫e incluso si hay problemas con las extensiones
    
    def serialize_vector(self, vector: List[float]) -> List[float]:
        """
        Convierte un vector para almacenamiento en DuckDB como FLOAT[].
        
        Args:
            vector: Lista de valores float que representan el vector
            
        Returns:
            Vector como lista de floats (para uso directo en DuckDB)
        """
        # Adaptar el vector a la dimensi√≥n configurada
        if len(vector) != self._embedding_dim:
            if len(vector) > self._embedding_dim:
                # Truncar si es m√°s grande
                vector = vector[:self._embedding_dim]
                logger.debug(f"Vector truncado a {self._embedding_dim} dimensiones")
            else:
                # Rellenar con ceros si es m√°s peque√±o
                vector = vector + [0.0] * (self._embedding_dim - len(vector))
                logger.debug(f"Vector rellenado a {self._embedding_dim} dimensiones")
        
        # Devolver como lista de floats para DuckDB FLOAT[]
        return vector
    
    def deserialize_vector(self, vector_array: List[float], dim: int = None) -> List[float]:
        """
        Obtiene un vector desde DuckDB FLOAT[].
        
        Args:
            vector_array: Vector como array de DuckDB
            dim: Dimensi√≥n del vector (se ignora y se usa la dimensi√≥n fija configurada)
            
        Returns:
            Lista de valores float que representan el vector
        """
        # Para DuckDB FLOAT[], simplemente devolver la lista
        return vector_array if vector_array else []

    def insert_document(self, document: Dict[str, Any], chunks: List[Dict[str, Any]]) -> int:
        """
        Inserta un documento y sus chunks en la base de datos.
        
        Los embeddings se adaptan autom√°ticamente a la dimensi√≥n fija configurada.
        
        Args:
            document: Diccionario con informaci√≥n del documento
            chunks: Lista de chunks generados a partir del documento
            
        Returns:
            ID del documento insertado
            
        Raises:
            Exception: Si hay un error durante la inserci√≥n
        """
        if not self._conn:
            raise ValueError("No hay conexi√≥n a la base de datos")
        
        if not self._schema_created:
            self.create_schema()
        
        try:
            # Comenzar una transacci√≥n para garantizar consistencia
            self.begin_transaction()
            
            # Insertar el documento y obtener su ID
            self._conn.execute("""
                INSERT INTO documents (title, url, file_path, created_at)
                VALUES (?, ?, ?, CURRENT_TIMESTAMP)
                RETURNING id
            """, (
                document.get('title', ''),
                document.get('url', ''),
                document.get('file_path', ''),
            ))
            
            # Obtener el ID del documento insertado usando RETURNING
            document_id = self._conn.fetchone()[0]
            
            # Insertar cada chunk
            for chunk in chunks:
                # Procesar el embedding
                embedding = chunk.get('embedding')
                
                # Convertir el embedding para DuckDB FLOAT[]
                processed_embedding = None
                if embedding is not None:
                    processed_embedding = self.serialize_vector(embedding)
                
                # Insertar el chunk con su embedding procesado
                self._conn.execute("""
                    INSERT INTO chunks (document_id, text, header, page, embedding, embedding_dim, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """, (
                    document_id,
                    chunk.get('text', ''),
                    chunk.get('header', None),
                    chunk.get('page', None),
                    processed_embedding,
                    self._embedding_dim if processed_embedding else None
                ))
            
            # Confirmar la transacci√≥n
            self.commit_transaction()
            
            logger.info(f"Documento insertado con ID {document_id} y {len(chunks)} chunks")
            
            return document_id
            
        except Exception as e:
            # Revertir los cambios en caso de error
            self.rollback_transaction()
            logger.error(f"Error al insertar documento: {e}")
            raise
    
    def insert_document_metadata(self, document: Dict[str, Any]) -> int:
        """
        Inserta solo los metadatos de un documento en la base de datos.
        Implementaci√≥n para procesamiento en streaming de documentos grandes.
        
        Args:
            document: Diccionario con los datos del documento
            
        Returns:
            int: ID del documento insertado, None si falla
        """
        if not self._conn:
            logger.error("No hay conexi√≥n a la base de datos")
            return None
        
        if not self._schema_created:
            self.create_schema()
        
        try:
            # Iniciar una transacci√≥n expl√≠cita
            self.begin_transaction()
            
            # Insertar el documento
            self._conn.execute("""
                INSERT INTO documents (title, url, file_path, created_at)
                VALUES (?, ?, ?, CURRENT_TIMESTAMP)
                RETURNING id
            """, (
                document.get('title', ''),
                document.get('url', ''),
                document.get('file_path', ''),
            ))
            
            # Obtener el ID del documento insertado usando RETURNING
            document_id = self._conn.fetchone()[0]
            
            # Hacer commit usando el m√©todo de la clase padre en lugar del comando SQL directo
            self.commit_transaction()
            
            logger.debug(f"Documento (solo metadatos) insertado con ID: {document_id}")
            return document_id
            
        except Exception as e:
            logger.error(f"Error al insertar metadatos del documento: {e}")
            # Hacer rollback en caso de error usando el m√©todo de la clase padre
            self.rollback_transaction()
            return None
    
    def insert_single_chunk(self, document_id: int, chunk: Dict[str, Any]) -> int:
        """
        Inserta un √∫nico chunk asociado a un documento en la base de datos.
        Dise√±ado para procesamiento streaming de documentos grandes.
        
        Args:
            document_id (int): ID del documento al que pertenece el chunk
            chunk (dict): Diccionario con los datos del chunk
                Debe contener: 'text', 'header', 'page', 'embedding', 'embedding_dim'
            
        Returns:
            int: ID del chunk insertado, None si falla
        """
        if not self._conn:
            logger.error("No hay conexi√≥n a la base de datos")
            return None
        
        try:
            # Procesar el embedding
            embedding = chunk.get('embedding')
            
            # Convertir el embedding para DuckDB FLOAT[]
            processed_embedding = None
            if embedding is not None:
                processed_embedding = self.serialize_vector(embedding)
            
            # Insertar el chunk individual
            self._conn.execute("""
                INSERT INTO chunks (document_id, text, header, page, embedding, embedding_dim, created_at)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                RETURNING id
            """, (
                document_id,
                chunk.get('text', ''),
                chunk.get('header', None),
                chunk.get('page', None),
                processed_embedding,
                self._embedding_dim if processed_embedding else None
            ))
            
            # Obtener el ID del chunk insertado
            chunk_id = self._conn.fetchone()[0]
            
            return chunk_id
            
        except Exception as e:
            logger.error(f"Error al insertar chunk individual: {e}")
            return None
    
    def optimize_database(self) -> bool:
        """
        Optimiza la base de datos realizando operaciones de mantenimiento.
        
        Returns:
            True si la optimizaci√≥n fue exitosa, False en caso contrario
        """
        if not self._conn:
            logger.error("No hay conexi√≥n a la base de datos")
            return False
        
        try:
            # Ejecutar vacuum para liberar espacio no utilizado
            self._conn.execute("VACUUM")
            
            # Analizar las tablas para mejorar el plan de consultas
            self._conn.execute("ANALYZE chunks")
            self._conn.execute("ANALYZE documents")
            
            logger.info("Base de datos DuckDB optimizada correctamente")
            return True
        except Exception as e:
            logger.error(f"Error al optimizar la base de datos: {e}")
            return False
    
    def vector_search(self, query_embedding: List[float], n_results: int = 5) -> List[Dict[str, Any]]:
        """
        Performs a vector search using cosine similarity with DuckDB FLOAT[] arrays.
        
        Args:
            query_embedding: Query embedding vector
            n_results: Number of results to return
            
        Returns:
            List of chunks with similarity scores
        """
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"üîç Starting vector search - query embedding dims: {len(query_embedding)}, requesting {n_results} results")
        
        if not query_embedding:
            logger.error("‚ùå Empty query embedding provided")
            return []
        
        try:
            # Check if we have any chunks
            count_query = "SELECT COUNT(*) as total FROM chunks WHERE embedding IS NOT NULL"
            result = self._conn.execute(count_query).fetchone()
            total_chunks = result[0] if result else 0
            
            logger.info(f"üìä Database contains {total_chunks} total chunks with embeddings")
            
            if total_chunks == 0:
                logger.warning("‚ö†Ô∏è No chunks with embeddings found in database")
                return []
            
            # Check dimensions of stored embeddings using array_length on FLOAT[]
            dim_query = "SELECT array_length(embedding) as dim FROM chunks WHERE embedding IS NOT NULL LIMIT 1"
            dim_result = self._conn.execute(dim_query).fetchone()
            stored_dim = dim_result[0] if dim_result else 0
            
            logger.info(f"üìè Stored embedding dimensions: {stored_dim}, Query dimensions: {len(query_embedding)}")
            
            if stored_dim != len(query_embedding):
                logger.error(f"‚ùå Dimension mismatch: stored={stored_dim}, query={len(query_embedding)}")
                return []
            
            # Convert query embedding to the format expected by DuckDB
            query_vector = self.serialize_vector(query_embedding)
            
            # Perform vector search query using list_cosine_similarity for FLOAT[] arrays
            search_query = """
            SELECT 
                c.id,
                c.text,
                c.header,
                c.page,
                d.title as document_title,
                list_cosine_similarity(c.embedding, ?::FLOAT[]) as similarity
            FROM chunks c
            JOIN documents d ON c.document_id = d.id
            WHERE c.embedding IS NOT NULL
            ORDER BY similarity DESC
            LIMIT ?
            """
            
            logger.info("üîÑ Executing vector search query...")
            results = self._conn.execute(search_query, [query_vector, n_results]).fetchall()
            
            logger.info(f"üìã Raw query returned {len(results)} results")
            
            # Format results
            chunks = []
            for row in results:
                chunk_id, text, header, page, doc_title, similarity = row
                
                logger.info(f"üìÑ Result {len(chunks)+1}: ID={chunk_id}, similarity={similarity:.3f}, text_preview='{text[:50]}...'")
                
                chunks.append({
                    'id': chunk_id,
                    'text': text,
                    'header': header or '',
                    'page': page or 'N/A',
                    'document_title': doc_title or '',
                    'similarity': float(similarity)
                })
            
            # Apply similarity threshold filtering
            threshold = getattr(self, '_similarity_threshold', 0.3)
            logger.info(f"üéØ Applying similarity threshold: {threshold}")
            
            filtered_chunks = [chunk for chunk in chunks if chunk['similarity'] >= threshold]
            
            if len(filtered_chunks) != len(chunks):
                logger.info(f"üîΩ Filtered from {len(chunks)} to {len(filtered_chunks)} chunks based on threshold")
            
            logger.info(f"‚úÖ Vector search completed - returning {len(filtered_chunks)} chunks")
            return filtered_chunks
            
        except Exception as e:
            logger.error(f"‚ùå Error in vector search: {e}", exc_info=True)
            return []

    def get_total_chunks_count(self) -> int:
        """
        Gets the total number of chunks in the database.
        
        Returns:
            Total number of chunks
        """
        try:
            result = self._conn.execute("SELECT COUNT(*) FROM chunks").fetchone()
            return result[0] if result else 0
        except Exception as e:
            logger.error(f"Error getting chunk count: {e}")
            return 0
    
    def _vector_search_manual(self, query_embedding: List[float], filters=None, n_results=5, include_neighbors=False):
        """
        Implementaci√≥n manual de b√∫squeda vectorial cuando no hay extensiones especializadas.
        
        Args:
            query_embedding: Vector de embedding de la consulta
            filters: Filtros para la b√∫squeda
            n_results: N√∫mero m√°ximo de resultados
            include_neighbors: Si se incluyen chunks vecinos en los resultados
        
        Returns:
            Lista de chunks ordenados por similitud
        """
        results = []
        
        try:
            # Serializar el embedding de consulta para comparaci√≥n
            query_embedding_blob = self.serialize_vector(query_embedding)
            
            # Construir la consulta base
            base_query = """
                SELECT c.id, c.document_id, c.text, c.header, c.page, c.embedding,
                       d.title, d.url, d.file_path
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.embedding IS NOT NULL
            """
            
            # A√±adir condiciones de filtro si existen
            params = []
            if filters:
                for key, value in filters.items():
                    if key == 'document_id':
                        base_query += " AND c.document_id = ?"
                        params.append(value)
                    elif key == 'header':
                        base_query += " AND c.header LIKE ?"
                        params.append(f"%{value}%")
                    # A√±adir m√°s filtros seg√∫n sea necesario
            
            # Ejecutar la consulta
            self._cursor.execute(base_query, params)
            all_chunks = self._cursor.fetchall()
            
            # Calcular similitud con cada chunk
            chunk_similarities = []
            for chunk in all_chunks:
                # Extraer los campos del chunk
                chunk_id = chunk[0]
                document_id = chunk[1]
                text = chunk[2]
                header = chunk[3]
                page = chunk[4]
                embedding_blob = chunk[5]
                title = chunk[6]
                url = chunk[7]
                file_path = chunk[8]
                
                # Deserializar el embedding
                if embedding_blob:
                    embedding = self.deserialize_vector(embedding_blob)
                    
                    # Calcular similitud por coseno
                    similarity = self._cosine_similarity(
                        np.array(query_embedding), 
                        np.array(embedding)
                    )
                    
                    # Filtrar por umbral de similitud
                    if similarity >= self._similarity_threshold:
                        chunk_similarities.append({
                            'id': chunk_id,
                            'document_id': document_id,
                            'text': text,
                            'header': header,
                            'page': page,
                            'title': title,
                            'url': url,
                            'file_path': file_path,
                            'similarity': float(similarity)
                        })
            
            # Ordenar por similitud descendente
            chunk_similarities.sort(key=lambda x: x['similarity'], reverse=True)
            
            # Limitar resultados
            results = chunk_similarities[:n_results]
            
            # Incluir chunks vecinos si se solicita
            if include_neighbors and results:
                for i, result in enumerate(results.copy()):
                    neighbors = self._get_adjacent_chunks(result['document_id'], result['id'])
                    
                    # A√±adir vecinos a los resultados, marc√°ndolos como vecinos
                    for neighbor in neighbors:
                        neighbor['is_neighbor'] = True
                        neighbor['neighbor_of'] = result['id']
                        # Asegurar que no estemos duplicando chunks
                        if not any(r['id'] == neighbor['id'] for r in results):
                            results.append(neighbor)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda vectorial: {e}")
        
        return results
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calcula la similitud de coseno entre dos vectores.
        
        Args:
            vec1: Primer vector
            vec2: Segundo vector
            
        Returns:
            Similitud de coseno (float entre -1 y 1)
        """
        # Manejar vectores vac√≠os
        if len(vec1) == 0 or len(vec2) == 0:
            return 0.0
            
        # Calcular normas
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        # Evitar divisi√≥n por cero
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        # Calcular similitud de coseno
        return np.dot(vec1, vec2) / (norm1 * norm2)
    
    def _get_adjacent_chunks(self, document_id: int, chunk_id: int) -> List[Dict[str, Any]]:
        """
        Obtiene los chunks adyacentes (anterior y siguiente) a un chunk dado.
        
        Args:
            document_id: ID del documento
            chunk_id: ID del chunk
            
        Returns:
            Lista de chunks adyacentes
        """
        if not self._conn:
            return []
            
        try:
            # Obtener el chunk anterior
            self._cursor.execute("""
                SELECT c.id, c.document_id, c.text, c.header, c.page, d.title, d.url, d.file_path
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.document_id = ? AND c.id < ?
                ORDER BY c.id DESC
                LIMIT 1
            """, (document_id, chunk_id))
            prev_chunk = self._cursor.fetchone()
            
            # Obtener el chunk siguiente
            self._cursor.execute("""
                SELECT c.id, c.document_id, c.text, c.header, c.page, d.title, d.url, d.file_path
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.document_id = ? AND c.id > ?
                ORDER BY c.id ASC
                LIMIT 1
            """, (document_id, chunk_id))
            next_chunk = self._cursor.fetchone()
            
            # Procesar los resultados
            adjacent_chunks = []
            
            if prev_chunk:
                adjacent_chunks.append({
                    'id': prev_chunk[0],
                    'document_id': prev_chunk[1],
                    'text': prev_chunk[2],
                    'header': prev_chunk[3],
                    'page': prev_chunk[4],
                    'title': prev_chunk[5],
                    'url': prev_chunk[6],
                    'file_path': prev_chunk[7],
                    'position': 'previous'
                })
                
            if next_chunk:
                adjacent_chunks.append({
                    'id': next_chunk[0],
                    'document_id': next_chunk[1],
                    'text': next_chunk[2],
                    'header': next_chunk[3],
                    'page': next_chunk[4],
                    'title': next_chunk[5],
                    'url': next_chunk[6],
                    'file_path': next_chunk[7],
                    'position': 'next'
                })
                
            return adjacent_chunks
            
        except Exception as e:
            logger.error(f"Error al obtener chunks adyacentes: {e}")
            return []
    
    def get_chunks_by_document(self, document_id: int, offset: int = 0, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Obtiene los chunks asociados a un documento espec√≠fico a partir de su ID.
        
        Args:
            document_id: ID del documento del cual se desean obtener los chunks.
            offset: Desfase para la paginaci√≥n (por defecto 0).
            limit: N√∫mero m√°ximo de chunks a retornar (por defecto 100).
            
        Returns:
            Lista de diccionarios, cada uno representando un chunk.
        """
        if not self._conn:
            logger.error("No hay conexi√≥n establecida en DuckDBVectorialDatabase")
            return []
        
        try:
            # Obtener chunks con la informaci√≥n del documento
            query = """
                SELECT c.id, c.document_id, c.text, c.header, c.page, c.embedding, 
                       d.title, d.url, d.file_path, c.created_at
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.document_id = ?
                ORDER BY c.id ASC
                LIMIT ? OFFSET ?
            """
            self._cursor.execute(query, (document_id, limit, offset))
            rows = self._cursor.fetchall()
            
            chunks = []
            for row in rows:
                chunk_id, doc_id, text, header, page, embedding_blob, title, url, file_path, created_at = row
                
                # Construir el diccionario del chunk
                chunk = {
                    "id": chunk_id,
                    "document_id": doc_id,
                    "text": text,
                    "header": header,
                    "page": page,
                    "title": title,
                    "url": url,
                    "file_path": file_path,
                    "created_at": created_at
                }
                
                # Si hay embedding, deserializarlo
                if embedding_blob is not None:
                    chunk["embedding"] = self.deserialize_vector(embedding_blob)
                
                chunks.append(chunk)
            
            logger.info(f"Recuperados {len(chunks)} chunks para el documento ID: {document_id} (offset: {offset}, limit: {limit})")
            return chunks
            
        except Exception as e:
            logger.error(f"Error en get_chunks_by_document: {e}")
            return []
    
    def batch_insert_chunks(self, chunks: List[Dict[str, Any]], document_id: int) -> bool:
        """
        Inserta m√∫ltiples chunks en la base de datos de forma optimizada.
        
        Args:
            chunks: Lista de diccionarios con informaci√≥n de chunks
            document_id: ID del documento al que pertenecen estos chunks
            
        Returns:
            True si la inserci√≥n fue exitosa, False en caso contrario
        """
        if not self._conn or not self._schema_created:
            logger.error("No hay conexi√≥n a la base de datos o el esquema no est√° creado")
            return False
            
        if not chunks:
            logger.warning("No hay chunks para insertar")
            return True
            
        try:
            # Comenzar transacci√≥n para optimizar rendimiento
            self.begin_transaction()
            
            # Preparar par√°metros para inserci√≥n masiva
            chunk_params = []
            
            for chunk in chunks:
                # Procesar el embedding
                embedding = chunk.get('embedding')
                processed_embedding = None
                
                if embedding is not None:
                    processed_embedding = self.serialize_vector(embedding)
                
                # Preparar par√°metros para este chunk
                chunk_params.append((
                    document_id,
                    chunk.get('text', ''),
                    chunk.get('header', None),
                    chunk.get('page', None),
                    processed_embedding,
                    self._embedding_dim if processed_embedding else None
                ))
            
            # Realizar inserci√≥n masiva
            if chunk_params:
                self._conn.executemany("""
                    INSERT INTO chunks (document_id, text, header, page, embedding, embedding_dim)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, chunk_params)
            
            # Confirmar transacci√≥n
            self.commit_transaction()
            
            logger.info(f"Insertados {len(chunks)} chunks en lote para el documento {document_id}")
            return True
        except Exception as e:
            # Revertir cambios en caso de error
            self.rollback_transaction()
            logger.error(f"Error al insertar chunks en lote: {e}")
            return False
            
    def convert_embedding_dimension(self, embedding: List[float], target_dim: int) -> np.ndarray:
        """
        Adapta la dimensi√≥n de un embedding al tama√±o deseado.
        
        Args:
            embedding: Vector original
            target_dim: Dimensi√≥n objetivo
            
        Returns:
            Vector redimensionado como numpy array
        """
        embedding_np = np.array(embedding, dtype=np.float32)
        current_dim = len(embedding_np)
        
        if current_dim == target_dim:
            return embedding_np
            
        logger.debug(f"Adaptando dimensi√≥n de embedding de {current_dim} a {target_dim}")
        
        if current_dim > target_dim:
            # Truncar si es m√°s grande
            return embedding_np[:target_dim]
        else:
            # Rellenar con ceros si es m√°s peque√±o
            padding = np.zeros(target_dim - current_dim, dtype=np.float32)
            return np.concatenate([embedding_np, padding])
            
    def fast_vector_search(self, query_embedding: List[float], n_results: int = 5) -> List[Dict[str, Any]]:
        """
        Versi√≥n optimizada de b√∫squeda vectorial para grandes vol√∫menes de datos.
        
        Utiliza t√©cnicas de procesamiento por lotes para mejorar el rendimiento.
        
        Args:
            query_embedding: Vector de embedding de la consulta
            n_results: N√∫mero m√°ximo de resultados a retornar
            
        Returns:
            Lista de chunks ordenados por similitud
        """
        if not self._conn:
            logger.error("No hay conexi√≥n a la base de datos")
            return []
        
        try:    
            # Normalizar y adaptar el embedding de consulta
            query_embedding_np = np.array(query_embedding, dtype=np.float32)
            query_norm = np.linalg.norm(query_embedding_np)
            if query_norm > 0:
                query_embedding_np = query_embedding_np / query_norm
                
            if len(query_embedding_np) != self._embedding_dim:
                query_embedding_np = self.convert_embedding_dimension(query_embedding, self._embedding_dim)
                
            # Serializar embedding para comparaci√≥n
            query_embedding_blob = self.serialize_vector(query_embedding_np.tolist())
            
            # Obtener todos los IDs de chunks con embeddings en un solo paso
            try:
                self._cursor.execute("""
                    SELECT id FROM chunks WHERE embedding IS NOT NULL
                """)
                chunk_ids = [row[0] for row in self._cursor.fetchall()]
                
                # Si no hay chunks, retornar lista vac√≠a
                if not chunk_ids:
                    logger.warning("No se encontraron chunks con embeddings en la base de datos")
                    return []
            except Exception as db_error:
                logger.error(f"Error al obtener IDs de chunks: {db_error}")
                return []
                
            # Procesar en lotes para evitar sobrecarga de memoria
            batch_size = 1000
            all_similarities = []
            total_processed = 0
            
            for i in range(0, len(chunk_ids), batch_size):
                batch_ids = chunk_ids[i:i+batch_size]
                placeholders = ', '.join(['?' for _ in batch_ids])
                
                try:
                    # Obtener embeddings para este lote
                    self._cursor.execute(f"""
                        SELECT c.id, c.document_id, c.text, c.header, c.page, c.embedding,
                               d.title, d.url, d.file_path
                        FROM chunks c
                        JOIN documents d ON c.document_id = d.id
                        WHERE c.id IN ({placeholders})
                    """, batch_ids)
                    
                    batch_results = self._cursor.fetchall()
                    
                    # Calcular similitudes para cada chunk en el lote
                    for row in batch_results:
                        if row is None or len(row) < 6:
                            continue
                            
                        chunk_id, doc_id, text, header, page, embedding_blob = row[:6]
                        title, url, file_path = row[6:9] if len(row) >= 9 else (None, None, None)
                        
                        if embedding_blob:
                            try:
                                # Deserializar embedding
                                embedding = self.deserialize_vector(embedding_blob)
                                
                                # Calcular similitud
                                similarity = self._cosine_similarity(
                                    query_embedding_np, 
                                    np.array(embedding, dtype=np.float32)
                                )
                                
                                # Agregar a resultados si supera el umbral
                                if similarity >= self._similarity_threshold:
                                    all_similarities.append({
                                        'id': chunk_id,
                                        'document_id': doc_id,
                                        'text': text,
                                        'header': header,
                                        'page': page,
                                        'title': title,
                                        'url': url,
                                        'file_path': file_path,
                                        'similarity': float(similarity)
                                    })
                            except Exception as vec_error:
                                logger.debug(f"Error procesando vector para chunk {chunk_id}: {vec_error}")
                                continue
                    
                    total_processed += len(batch_results)
                    
                except Exception as batch_error:
                    logger.error(f"Error procesando lote de chunks {i}-{i+batch_size}: {batch_error}")
                    # Continuar con el siguiente lote
            
            # Log informativo sobre el procesamiento
            logger.info(f"Procesados {total_processed} chunks en b√∫squeda vectorial. Encontrados {len(all_similarities)} resultados relevantes.")
            
            # Ordenar por similitud y limitar resultados
            all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
            return all_similarities[:n_results]
                
        except Exception as e:
            logger.error(f"Error en b√∫squeda vectorial r√°pida: {e}")
            return []
    
    def store_metadata(self, key: str, value: Any) -> bool:
        """
        Almacena un metadato en la base de datos.
        
        Args:
            key: Clave del metadato
            value: Valor del metadato
            
        Returns:
            True si se almacen√≥ correctamente, False en caso contrario
        """
        if not self._conn:
            logger.warning(f"No se puede almacenar metadato '{key}': no hay conexi√≥n a la base de datos")
            
            # Si tenemos una ruta de base de datos guardada, intentar reconectar
            if self._db_path:
                logger.info(f"Intentando reconectar a la base de datos {self._db_path} para guardar metadatos")
                if self.connect(self._db_path):
                    logger.info("Reconexi√≥n exitosa, continuando con el almacenamiento de metadatos")
                else:
                    logger.error(f"No se pudo reconectar a la base de datos {self._db_path}")
                    return False
            else:
                return False
            
        try:
            # Verificar si la tabla existe
            try:
                self._conn.execute("SELECT 1 FROM db_metadata LIMIT 1")
            except Exception:
                # Si hay error, crear la tabla
                self._conn.execute("""
                    CREATE TABLE IF NOT EXISTS db_metadata (
                        key TEXT PRIMARY KEY,
                        value TEXT,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    );
                """)
            
            # Convertir el valor a string si es necesario
            if not isinstance(value, str):
                if isinstance(value, (int, float, bool, type(None))):
                    value = str(value)
                else:
                    value = json.dumps(value)
            
            # Insertar o actualizar el metadato
            self._conn.execute("""
                INSERT OR REPLACE INTO db_metadata (key, value, updated_at)
                VALUES (?, ?, CURRENT_TIMESTAMP);
            """, (key, value))
            
            return True
        except Exception as e:
            logger.error(f"Error al almacenar metadato '{key}': {e}")
            return False
    
    def _add_neighbors_to_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        A√±ade chunks vecinos a los resultados de b√∫squeda.
        
        Args:
            results: Lista de resultados de b√∫squeda
            
        Returns:
            Lista aumentada con chunks vecinos
        """
        if not results:
            return results
            
        final_results = results.copy()
        
        for result in results:
            neighbors = self._get_adjacent_chunks(result['document_id'], result['id'])
            
            # A√±adir vecinos a los resultados, marc√°ndolos como vecinos
            for neighbor in neighbors:
                neighbor['is_neighbor'] = True
                neighbor['neighbor_of'] = result['id']
                # Asegurar que no estemos duplicando chunks
                if not any(r['id'] == neighbor['id'] for r in final_results):
                    final_results.append(neighbor)
        
        return final_results
    
    def begin_transaction(self) -> bool:
        """
        Inicia una transacci√≥n en DuckDB.
        
        Returns:
            bool: True si se inici√≥ correctamente, False en caso contrario
        """
        try:
            if not self._conn:
                logger.error("No hay conexi√≥n a la base de datos para iniciar transacci√≥n")
                return False
            
            # Verificar si ya hay una transacci√≥n activa
            if self._in_transaction:
                logger.debug("Ya hay una transacci√≥n activa en DuckDB, ignorando begin_transaction")
                return True  # Ya estamos en una transacci√≥n, no es un error
                
            # Iniciar transacci√≥n en DuckDB
            self._conn.execute("BEGIN TRANSACTION")
            self._in_transaction = True
            logger.debug("Transacci√≥n iniciada en DuckDB")
            return True
        except Exception as e:
            logger.error(f"Error al iniciar transacci√≥n en DuckDB: {e}")
            return False
    
    def commit_transaction(self) -> bool:
        """
        Confirma una transacci√≥n activa en DuckDB.
        
        Returns:
            bool: True si se confirm√≥ correctamente, False en caso contrario
        """
        try:
            if not self._conn:
                logger.error("No hay conexi√≥n a la base de datos para confirmar transacci√≥n")
                return False
            
            # Verificar si hay una transacci√≥n activa para confirmar
            if not self._in_transaction:
                logger.debug("No hay transacci√≥n activa en DuckDB para confirmar")
                return True  # No hay transacci√≥n que confirmar, no es un error
                
            # Confirmar transacci√≥n en DuckDB
            self._conn.execute("COMMIT")
            self._in_transaction = False
            logger.debug("Transacci√≥n confirmada en DuckDB")
            return True
        except Exception as e:
            logger.error(f"Error al confirmar transacci√≥n en DuckDB: {e}")
            # Intentar hacer rollback en caso de error
            try:
                self._conn.execute("ROLLBACK")
                self._in_transaction = False
            except:
                pass
            return False
    
    def rollback_transaction(self) -> bool:
        """
        Revierte una transacci√≥n activa en DuckDB.
        
        Returns:
            bool: True si se revirti√≥ correctamente, False en caso contrario
        """
        try:
            if not self._conn:
                logger.error("No hay conexi√≥n a la base de datos para revertir transacci√≥n")
                return False
            
            # Verificar si hay una transacci√≥n activa para revertir
            if not self._in_transaction:
                logger.debug("No hay transacci√≥n activa en DuckDB para revertir")
                return True  # No hay transacci√≥n que revertir, no es un error
                
            # Revertir transacci√≥n en DuckDB
            self._conn.execute("ROLLBACK")
            self._in_transaction = False
            logger.debug("Transacci√≥n revertida en DuckDB")
            return True
        except Exception as e:
            logger.error(f"Error al revertir transacci√≥n en DuckDB: {e}")
            return False